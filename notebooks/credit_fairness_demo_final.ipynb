{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Credit Fairness Demo (final)\n",
        "\n",
        "End-to-end walkthrough: generate synthetic credit underwriting data, train GLM / NN / ADV_NN, and reproduce the auto fairness figures (lambda sweep, fairness vs accuracy, fairness vs high-risk rate, and score distributions) on the credit data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if not (PROJECT_ROOT / \"src\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Using project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from dataclasses import replace\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from src.config import get_default_configs\n",
        "from src.credit import generate_credit_underwriting_data, train_test_split_df\n",
        "from src.evaluation.metrics import compute_accuracy_metrics\n",
        "from src.evaluation.fairness import fairness_metrics, fairness_at_target_rate\n",
        "from src.models.glm_model import GLMClassifier\n",
        "from src.models.nn_model import PlainNN, train_plain_nn, predict_proba_plain_nn\n",
        "from src.models.adv_nn_model import AdvPredictor, train_adv_nn, predict_proba_adv_nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "try:\n",
        "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Credit Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_cfg, train_cfg, eval_cfg = get_default_configs()\n",
        "sim_cfg = replace(sim_cfg, n_samples=100_000, seed=SEED)\n",
        "\n",
        "df_full = generate_credit_underwriting_data(sim_cfg)\n",
        "print(f\"Dataset size: {len(df_full):,}\")\n",
        "df_full.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split_df(df_full, test_size=0.2, seed=sim_cfg.seed)\n",
        "print(f\"Train size: {len(df_train):,}, Test size: {len(df_test):,}\")\n",
        "df_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = [\"S\", \"D\", \"L\"]\n",
        "proxy_col = \"Z\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_num = scaler.fit_transform(df_train[numeric_cols])\n",
        "X_test_num = scaler.transform(df_test[numeric_cols])\n",
        "\n",
        "X_train = np.concatenate(\n",
        "    [X_train_num, df_train[[proxy_col]].to_numpy()],\n",
        "    axis=1\n",
        ").astype(np.float32)\n",
        "X_test = np.concatenate(\n",
        "    [X_test_num, df_test[[proxy_col]].to_numpy()],\n",
        "    axis=1\n",
        ").astype(np.float32)\n",
        "\n",
        "y_train = df_train[\"Y\"].to_numpy(dtype=np.float32)\n",
        "y_test = df_test[\"Y\"].to_numpy(dtype=np.float32)\n",
        "A_train = df_train[\"A\"].to_numpy(dtype=np.int64)\n",
        "A_test = df_test[\"A\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=sim_cfg.seed, stratify=y_train\n",
        ")\n",
        "\n",
        "def build_loader(X, y, batch_size, shuffle=True):\n",
        "    ds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = build_loader(X_tr, y_tr, train_cfg.batch_size, shuffle=True)\n",
        "val_loader = build_loader(X_val, y_val, train_cfg.batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train.shape, X_test.shape, device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GLM Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "glm = GLMClassifier().fit(X_train, y_train)\n",
        "y_proba_glm = glm.predict_proba(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plain_nn = PlainNN(input_dim=X_train.shape[1]).to(device)\n",
        "train_plain_nn(plain_nn, train_loader, val_loader, train_cfg, device)\n",
        "y_proba_nn = predict_proba_plain_nn(plain_nn, X_test, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Adversarial Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_cfg_adv = replace(train_cfg, lambda_adv=0.8)\n",
        "\n",
        "adv_ds = TensorDataset(\n",
        "    torch.from_numpy(X_train).float(),\n",
        "    torch.from_numpy(y_train).float(),\n",
        "    torch.from_numpy(A_train).long(),\n",
        ")\n",
        "adv_loader = DataLoader(adv_ds, batch_size=train_cfg.batch_size, shuffle=True)\n",
        "\n",
        "adv_model = AdvPredictor(input_dim=X_train.shape[1]).to(device)\n",
        "train_adv_nn(adv_model, adv_loader, train_cfg_adv, device=device)\n",
        "y_proba_adv = predict_proba_adv_nn(adv_model, X_test, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sweep lambda for ADV_NN and pick the fairest model at 2% target rate\n",
        "lambda_grid = [0.1, 0.3, 0.8, 1.5, 2.0]\n",
        "frontier_rows = []\n",
        "best_row = None\n",
        "best_proba = None\n",
        "for lam in lambda_grid:\n",
        "    cfg_lam = replace(train_cfg, lambda_adv=lam)\n",
        "    model_lam = AdvPredictor(input_dim=X_train.shape[1]).to(device)\n",
        "    train_adv_nn(model_lam, adv_loader, cfg_lam, device=device)\n",
        "    y_proba_lam = predict_proba_adv_nn(model_lam, X_test, device=device)\n",
        "    acc = compute_accuracy_metrics(y_test, y_proba_lam)\n",
        "    fair = fairness_at_target_rate(y_test, y_proba_lam, A_test, target_rate=0.02)\n",
        "    row = {'model': 'ADV_NN', 'lambda_adv': lam, **acc, **fair}\n",
        "    frontier_rows.append(row)\n",
        "    score = (fair['eo_gap_tpr'], fair['eo_gap_fpr'], fair['dp_ratio'])\n",
        "    if best_row is None or score < (best_row['eo_gap_tpr'], best_row['eo_gap_fpr'], best_row['dp_ratio']):\n",
        "        best_row = row\n",
        "        best_proba = y_proba_lam\n",
        "df_frontier_02 = pd.DataFrame(frontier_rows)\n",
        "y_proba_adv = best_proba\n",
        "best_lambda_adv = float(best_row['lambda_adv'])\n",
        "print(f\"Selected ADV lambda={best_lambda_adv:.2f} with EO gap {best_row['eo_gap_tpr']:.4f} and ROC AUC {best_row['roc_auc']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation & Fairness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(model, y_true, y_proba, A_true, threshold=eval_cfg.threshold):\n",
        "    acc = compute_accuracy_metrics(y_true, y_proba)\n",
        "    fair = fairness_metrics(y_true, y_proba, A_true, threshold=threshold)\n",
        "    return {\"model\": model, **acc, **fair}\n",
        "\n",
        "summary_default = pd.DataFrame(\n",
        "    [\n",
        "        summarize(\"GLM\", y_test, y_proba_glm, A_test),\n",
        "        summarize(\"NN\", y_test, y_proba_nn, A_test),\n",
        "        summarize(\"ADV_NN\", y_test, y_proba_adv, A_test),\n",
        "    ]\n",
        ")\n",
        "summary_default\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target 0.02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_RATE = 0.02\n",
        "\n",
        "def summarize_fixed(model, y_true, y_proba, A_true):\n",
        "    fair = fairness_at_target_rate(y_true, y_proba, A_true, TARGET_RATE)\n",
        "    acc = compute_accuracy_metrics(y_true, y_proba)\n",
        "    return {\"model\": model, **acc, **fair}\n",
        "\n",
        "summary_fixed = pd.DataFrame(\n",
        "    [\n",
        "        summarize_fixed(\"GLM\", y_test, y_proba_glm, A_test),\n",
        "        summarize_fixed(\"NN\", y_test, y_proba_nn, A_test),\n",
        "        summarize_fixed(\"ADV_NN\", y_test, y_proba_adv, A_test),\n",
        "    ]\n",
        ")\n",
        "summary_fixed[\n",
        "    [\"model\", \"roc_auc\", \"eo_gap_tpr\", \"eo_gap_fpr\", \"dp_diff\", \"dp_ratio\", \"threshold\", \"actual_rate\"]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Credit experiment plots\n",
        "\n",
        "Use the GLM/NN/ADV_NN predictions from this notebook to visualize fairness vs accuracy and fairness vs high-risk rate. ADV_NN uses the best lambda from the sweep above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from src.evaluation.fairness import fairness_at_target_rate\n",
        "from src.evaluation.metrics import compute_accuracy_metrics\n",
        "\n",
        "target_rates = [0.01, 0.02, 0.05]\n",
        "colors = {'GLM': 'tab:orange', 'NN': 'tab:green', 'ADV_NN': 'tab:purple'}\n",
        "\n",
        "def summarize_at_rates(model, y_proba):\n",
        "    acc = compute_accuracy_metrics(y_test, y_proba)\n",
        "    rows = []\n",
        "    for r in target_rates:\n",
        "        fair = fairness_at_target_rate(y_test, y_proba, A_test, r)\n",
        "        rows.append({'model': model, **acc, **fair})\n",
        "    return rows\n",
        "\n",
        "rows = []\n",
        "rows += summarize_at_rates('GLM', y_proba_glm)\n",
        "rows += summarize_at_rates('NN', y_proba_nn)\n",
        "rows += summarize_at_rates('ADV_NN', y_proba_adv)\n",
        "df_rates = pd.DataFrame(rows)\n",
        "summary_fixed = df_rates[df_rates['target_rate'] == 0.02].reset_index(drop=True)\n",
        "df_frontier_plot = pd.concat([\n",
        "    df_frontier_02[['model','lambda_adv','eo_gap_tpr','roc_auc']],\n",
        "    summary_fixed.assign(lambda_adv=np.nan)[['model','lambda_adv','eo_gap_tpr','roc_auc']]\n",
        "])\n",
        "display(summary_fixed[['model','roc_auc','eo_gap_tpr','eo_gap_fpr','dp_diff','dp_ratio','threshold','actual_rate']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 ADV lambda sweep (2% high-risk)\n",
        "\n",
        "EO gap vs ROC AUC for ADV_NN across lambdas, with GLM/NN anchors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "lambda_pts = df_frontier_plot[df_frontier_plot['model']=='ADV_NN']\n",
        "if not lambda_pts.empty:\n",
        "    ax.scatter(lambda_pts['eo_gap_tpr'], lambda_pts['roc_auc'], c='tab:blue', label='ADV lambda sweep')\n",
        "    for _, row in lambda_pts.iterrows():\n",
        "        if pd.notna(row.get('lambda_adv')):\n",
        "            ax.annotate(f\"lambda={row['lambda_adv']}\", (row['eo_gap_tpr'], row['roc_auc']), textcoords='offset points', xytext=(5,5), fontsize=8)\n",
        "for model in ['GLM','NN']:\n",
        "    subset = summary_fixed[summary_fixed['model']==model]\n",
        "    if subset.empty:\n",
        "        continue\n",
        "    row = subset.iloc[0]\n",
        "    ax.scatter(row['eo_gap_tpr'], row['roc_auc'], c=colors.get(model, 'tab:gray'), marker='D', label=model)\n",
        "    ax.annotate(model, (row['eo_gap_tpr'], row['roc_auc']), textcoords='offset points', xytext=(5,-10), fontsize=9, fontweight='bold', color=colors.get(model, 'black'))\n",
        "ax.set_xlabel('EO TPR difference @ 2% high-risk')\n",
        "ax.set_ylabel('ROC AUC')\n",
        "ax.set_title('Fairness vs Accuracy (2% high-risk) - lambda sweep')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Fairness vs accuracy (2% high-risk, chosen lambda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "for _, row in summary_fixed.iterrows():\n",
        "    ax.scatter(row['eo_gap_tpr'], row['roc_auc'], color=colors.get(row['model'],'tab:blue'), marker='D', label=row['model'])\n",
        "    ax.annotate(row['model'], (row['eo_gap_tpr'], row['roc_auc']), textcoords='offset points', xytext=(5,-10), fontsize=9, fontweight='bold', color=colors.get(row['model'],'black'))\n",
        "ax.set_title('Fairness vs Accuracy (2% high-risk)')\n",
        "ax.set_xlabel('EO TPR difference @2%')\n",
        "ax.set_ylabel('ROC AUC')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Fairness vs high-risk rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "for model, color in colors.items():\n",
        "    subset = df_rates[df_rates['model']==model].sort_values('target_rate')\n",
        "    ax.plot(subset['target_rate'], subset['dp_ratio'], marker='o', color=color, label=model)\n",
        "ax.set_xlabel('High-risk rate')\n",
        "ax.set_ylabel('DP ratio')\n",
        "ax.set_title('DP ratio vs high-risk rate')\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "for model, color in colors.items():\n",
        "    subset = df_rates[df_rates['model']==model].sort_values('target_rate')\n",
        "    ax.plot(subset['target_rate'], subset['eo_gap_tpr'], marker='o', color=color, label=model)\n",
        "ax.set_xlabel('High-risk rate')\n",
        "ax.set_ylabel('EO TPR difference')\n",
        "ax.set_title('EO gap vs high-risk rate')\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.4 Score distributions by group (GLM vs ADV_NN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "for ax, group in zip(axes, [0, 1]):\n",
        "    mask = (A_test == group)\n",
        "    all_scores = np.concatenate([y_proba_glm[mask], y_proba_adv[mask]])\n",
        "    bins = np.linspace(all_scores.min(), all_scores.max(), 51)\n",
        "    ax.hist(y_proba_glm[mask], bins=bins, alpha=0.5, label=f'GLM A={group}')\n",
        "    ax.hist(y_proba_adv[mask], bins=bins, alpha=0.5, label=f'ADV A={group}')\n",
        "    ax.set_title(f'Predicted probability distribution (A={group})')\n",
        "    ax.set_xlabel('Predicted probability')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Simulated variable distributions\n",
        "\n",
        "Quick check of the generated credit features and targets using the df_full dataset from earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_pairs_1 = ['S_star', 'S']\n",
        "numeric_pairs_2 = ['D', 'L']\n",
        "binary_cols = ['A', 'Z', 'Y']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "axes = axes.ravel()\n",
        "axes[0].hist(df_full[numeric_pairs_1[0]], bins=40, color='tab:blue', alpha=0.6)\n",
        "axes[0].set_title(numeric_pairs_1[0])\n",
        "axes[1].hist(df_full[numeric_pairs_1[1]], bins=40, color='tab:orange', alpha=0.6)\n",
        "axes[1].set_title(numeric_pairs_1[1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
        "axes = axes.ravel()\n",
        "axes[0].hist(df_full[numeric_pairs_2[0]], bins=40, color='tab:green', alpha=0.7)\n",
        "axes[0].set_title(numeric_pairs_2[0])\n",
        "axes[1].hist(df_full[numeric_pairs_2[1]], bins=40, color='tab:purple', alpha=0.7)\n",
        "axes[1].set_title(numeric_pairs_2[1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, len(binary_cols), figsize=(3 * len(binary_cols), 3))\n",
        "for ax, col in zip(axes, binary_cols):\n",
        "    counts = df_full[col].value_counts().reindex([0, 1], fill_value=0)\n",
        "    ax.bar(counts.index.astype(str), counts.values, color='tab:cyan', alpha=0.7)\n",
        "    ax.set_title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fairness_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}