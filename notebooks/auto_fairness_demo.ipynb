{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ef1aa7",
   "metadata": {},
   "source": [
    "# Auto Fairness Demo\n",
    "\n",
    "End-to-end walkthrough: generate synthetic auto underwriting data, train GLM / NN / ADV_NN, and evaluate fairness at both default and fixed approval thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b0ebe",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dataclasses import replace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.auto.data_generation_auto import generate_auto_underwriting_data, train_test_split_auto\n",
    "from src.config import TrainingConfig\n",
    "from src.evaluation.metrics import compute_accuracy_metrics\n",
    "from src.evaluation.fairness import fairness_metrics, fairness_at_target_rate\n",
    "from src.models.glm_model import GLMClassifier\n",
    "from src.models.nn_model import PlainNN, train_plain_nn, predict_proba_plain_nn\n",
    "from src.models.adv_nn_model import AdvPredictor, train_adv_nn, predict_proba_adv_nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f16c7",
   "metadata": {},
   "source": [
    "## 2. Generate Auto Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_auto_underwriting_data(n_samples=100_000, seed=123)\n",
    "print(f\"Dataset size: {len(df):,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68a478",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98128e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split_auto(df, seed=123)\n",
    "print(f\"Train size: {len(df_train):,}, Test size: {len(df_test):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d70f16",
   "metadata": {},
   "source": [
    "### Feature preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_cont = [\"Age\", \"M_h\", \"V\"]\n",
    "binary_cols = [\"P_prior\", \"T\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_cont = scaler.fit_transform(df_train[feature_cols_cont])\n",
    "X_test_cont = scaler.transform(df_test[feature_cols_cont])\n",
    "\n",
    "X_train = np.concatenate(\n",
    "    [X_train_cont, df_train[binary_cols].to_numpy()],\n",
    "    axis=1\n",
    ").astype(np.float32)\n",
    "X_test = np.concatenate(\n",
    "    [X_test_cont, df_test[binary_cols].to_numpy()],\n",
    "    axis=1\n",
    ").astype(np.float32)\n",
    "\n",
    "y_train = df_train[\"Y\"].to_numpy(dtype=np.float32)\n",
    "y_test = df_test[\"Y\"].to_numpy(dtype=np.float32)\n",
    "A_train = (df_train[\"Race\"] == \"A\").astype(int).to_numpy(dtype=np.int64)\n",
    "A_test = (df_test[\"Race\"] == \"A\").astype(int).to_numpy(dtype=np.int64)\n",
    "\n",
    "youth_train = (df_train[\"Age\"] < 25).astype(int).to_numpy(dtype=np.float32)\n",
    "youth_test = (df_test[\"Age\"] < 25).astype(int).to_numpy(dtype=np.float32)\n",
    "X_train_glm = np.concatenate([X_train, youth_train[:, None]], axis=1)\n",
    "X_test_glm = np.concatenate([X_test, youth_test[:, None]], axis=1)\n",
    "\n",
    "train_cfg = TrainingConfig()\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=123, stratify=y_train\n",
    ")\n",
    "\n",
    "def build_loader(X, y, batch_size, shuffle=True):\n",
    "    ds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = build_loader(X_tr, y_tr, train_cfg.batch_size, shuffle=True)\n",
    "val_loader = build_loader(X_val, y_val, train_cfg.batch_size, shuffle=False)\n",
    "test_loader = build_loader(X_test, y_test, train_cfg.batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train.shape, X_test.shape, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ad4f2",
   "metadata": {},
   "source": [
    "## 4. GLM Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = GLMClassifier().fit(X_train_glm, y_train)\n",
    "y_proba_glm = glm.predict_proba(X_test_glm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af9d51",
   "metadata": {},
   "source": [
    "## 5. Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdfd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_nn = PlainNN(input_dim=X_train.shape[1]).to(device)\n",
    "train_plain_nn(plain_nn, train_loader, val_loader, train_cfg, device)\n",
    "y_proba_nn = predict_proba_plain_nn(plain_nn, X_test, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f21a52",
   "metadata": {},
   "source": [
    "## 6. Adversarial Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e15871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg_adv = replace(train_cfg, lambda_adv=0.8)\n",
    "\n",
    "adv_ds = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    "    torch.from_numpy(A_train).long(),\n",
    ")\n",
    "adv_loader = DataLoader(adv_ds, batch_size=train_cfg.batch_size, shuffle=True)\n",
    "\n",
    "adv_model = AdvPredictor(input_dim=X_train.shape[1]).to(device)\n",
    "train_adv_nn(adv_model, adv_loader, train_cfg_adv, device=device)\n",
    "y_proba_adv = predict_proba_adv_nn(adv_model, X_test, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c6c98",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, y_true, y_proba, A_true, threshold=0.5):\n",
    "    acc = compute_accuracy_metrics(y_true, y_proba)\n",
    "    fair = fairness_metrics(y_true, y_proba, A_true, threshold=threshold)\n",
    "    return {\"model\": model, **acc, **fair}\n",
    "\n",
    "summary_default = pd.DataFrame(\n",
    "    [\n",
    "        summarize(\"GLM\", y_test, y_proba_glm, A_test),\n",
    "        summarize(\"NN\", y_test, y_proba_nn, A_test),\n",
    "        summarize(\"ADV_NN\", y_test, y_proba_adv, A_test),\n",
    "    ]\n",
    ")\n",
    "summary_default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfded1",
   "metadata": {},
   "source": [
    "### Target 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e73648",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_RATE = 0.02\n",
    "\n",
    "def summarize_fixed(model, y_true, y_proba, A_true):\n",
    "    fair = fairness_at_target_rate(y_true, y_proba, A_true, TARGET_RATE)\n",
    "    acc = compute_accuracy_metrics(y_true, y_proba)\n",
    "    return {\"model\": model, **acc, **fair}\n",
    "\n",
    "summary_fixed = pd.DataFrame(\n",
    "    [\n",
    "        summarize_fixed(\"GLM\", y_test, y_proba_glm, A_test),\n",
    "        summarize_fixed(\"NN\", y_test, y_proba_nn, A_test),\n",
    "        summarize_fixed(\"ADV_NN\", y_test, y_proba_adv, A_test),\n",
    "    ]\n",
    ")\n",
    "summary_fixed[\n",
    "    [\"model\", \"roc_auc\", \"eo_gap_tpr\", \"eo_gap_fpr\", \"dp_diff\", \"dp_ratio\", \"threshold\", \"actual_rate\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be247e1",
   "metadata": {},
   "source": [
    "## 8. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"### Key Insights\\n\"\n",
    "    f\"- GLM ROC AUC {summary_fixed.loc[summary_fixed.model=='GLM', 'roc_auc'].iat[0]:.3f}; \"\n",
    "    f\"EO gap {summary_fixed.loc[summary_fixed.model=='GLM', 'eo_gap_tpr'].iat[0]:.3f}; \"\n",
    "    f\"DP ratio {summary_fixed.loc[summary_fixed.model=='GLM', 'dp_ratio'].iat[0]:.2f} at 2% approval.\\n\"\n",
    "    f\"- NN ROC AUC {summary_fixed.loc[summary_fixed.model=='NN', 'roc_auc'].iat[0]:.3f}; \"\n",
    "    f\"EO gap {summary_fixed.loc[summary_fixed.model=='NN', 'eo_gap_tpr'].iat[0]:.3f}; \"\n",
    "    f\"DP ratio {summary_fixed.loc[summary_fixed.model=='NN', 'dp_ratio'].iat[0]:.2f} with modest fairness changes.\\n\"\n",
    "    f\"- ADV_NN ROC AUC {summary_fixed.loc[summary_fixed.model=='ADV_NN', 'roc_auc'].iat[0]:.3f}; \"\n",
    "    f\"EO gap {summary_fixed.loc[summary_fixed.model=='ADV_NN', 'eo_gap_tpr'].iat[0]:.3f}; \"\n",
    "    f\"DP ratio {summary_fixed.loc[summary_fixed.model=='ADV_NN', 'dp_ratio'].iat[0]:.2f} reflecting adversarial mitigation.\\n\"\n",
    ")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Auto experiment plots\n",
    "\n",
    "Use the auto bias sweep outputs to visualize fairness vs accuracy and fairness vs injected bias strength. Run `src/experiments/auto/bias_sweep.py` first to populate `results/auto/*/bias_sweep_metrics.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.auto import plot_fairness_accuracy_frontier as auto_frontier\n",
    "from src.experiments.auto import plot_fairness_vs_rate as auto_vs_rate\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Fairness vs accuracy frontier\n",
    "\n",
    "Scatter EO gap vs ROC AUC across bias strengths for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_frontier_path = auto_frontier.main()\n",
    "display(Image(filename=auto_frontier_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Fairness vs bias strength\n",
    "\n",
    "Plot DP ratio and EO gap as the injected bias strength varies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_bias_path, eo_bias_path = auto_vs_rate.main()\n",
    "display(Image(filename=dp_bias_path))\n",
    "display(Image(filename=eo_bias_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}